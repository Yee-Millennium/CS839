\documentclass[11pt]{article}

\usepackage{preamble}
\input{math_commands.tex}

\title{Notes of CS 839: Advanced Nonlinear Optimization\\Instructor: Jelena Diakonikolas}
\author{YI WEI}
\date{Sep 2024}


\begin{document}

\maketitle
\tableofcontents

\section{Vector Space}

\yee{TODO: Notes of Sep 4.}

\DATE{Sep 6, 2024}
\begin{example}
    \begin{enumerate}
        \item Induced matrix norms 
                $A \in \R^{m \times n} $
                Let $\| \cdot \|_{a} $ be any norm in $\mathbb{R}^n, \| \cdot  \| _{b}$ be any norm in $R^m$,
                $\| A \|_{a,b} = \max_{x \in \mathbb{R}^n: \| x\|_{a} \le 1 } \| Ax \|_{b} $ \\
                In particular, if $\| \cdot \|_{a} $ and $\| \cdot \|_{b}$ are $l_{p}$ norms:
                \begin{enumerate}
                    \item $a=b=2 \to $ operator/spectral norm
                    \item $a=b=1$:
                            \begin{align}
                                \| A \|_{1,1} &= \max_{x \in \mathbb{R}^n, \| x \|_{1} \le 1} \| Ax \|_{1} \\
                                    &= \max_{1 \le j \le n}\sum_{i=1}^{n}| A_{ij} |
                            \end{align}
                            It's called "max abs column sum"
                    \item $a=b=\infty$:
                            \begin{align*}
                                \| A \| _{\infty,\infty} = \max_{x \in \mathbb{R}^n, \| x \| _{\infty} \le 1}
                                        \| Ax \| _{\infty} = \max_{1 \le i \le m} \sum_{j=1}^{n}|A_{ij}|
                            \end{align*}
                            It's called "max abs row sum norm". 
                    \item $a=1, b=\infty$:
                            \begin{align*}
                                \| A \| _{1,\infty} = \max_{x\in \mathbb{R}^n, \| x \| _{1} \le 1} \| Ax \|_{\infty}
                                        = \max_{1 \le i \le m, 1 \le j \le n} |A_{ij}|
                            \end{align*}
                            where $\| Ax \| _{\infty} = \begin{bmatrix} A_{1}x \\ A_{2}x \\ \vdots \\ A_{n}x \end{bmatrix}$
                \end{enumerate}
    \end{enumerate}
\end{example}

\subsection{Cartesian Product of Vector Space}
Given $m \ge 2$ vector spaces $\mathbb{E}_1, \ldots , \mathbb{E}_{m}$ equipped w/ inner products 
$\langle \cdot, \cdot  \rangle, \ldots , \langle \cdot, \cdot \rangle$, their Cartesian product is the vector
space $\mathbb{E} = \mathbb{E}_1 \times \cdots \times \mathbb{E}_{n}$ containing all m-tuples $( \vv_1, \ldots ,\vv_m ) $ for which basic operations are defined as:
\begin{enumerate}
    \item Addition: $( \vv_{1}, \ldots ,\vv_m ) + ( \vw_1, \ldots ,\vw_{m} ) $ = 
    \item Scaler multiplication: $\alpha \in \mathbb{R}, \alpha (\vv_{1}, \ldots ,\vv_{m}) = 
            (\alpha \vv_{1}, \ldots , \alpha \vv_{m})$
\end{enumerate}
The inner product on $\mathbb{E}$ is defined by:
\begin{align*}
    \langle (\vv_{1}, \ldots ,\vv_{m}), (\vw_{1}, \ldots ,\vw_{m}) \rangle = \sum_{i=1}^{m}\langle v_{i}, w_{i} \rangle _{\mathbb{E}_{i}}
\end{align*}

If $\mathbb{E}_{i}, i \in \{ 1, \ldots ,m \} $ are endowed w/ norms $\|  \| _{E_{i}}$ there a different 
ways of choosing a norm on $\mathbb{E}$
\begin{example}
    \item $\| (\vv_{1}, \ldots ,\vv_{m}) \| = (\sum_{i=1}^{m}\| v_{i} \|_{\mathbb{E}_{i}}^p)^{\frac{1}{p}} $
    \item $\| (\vv_1, \ldots ,\vv_{m}) \| = ( \sum_{i=1}^{m} w_{i}\| v_{i} \|_{\mathbb{E}_{i}}^2  ) $
\end{example}


\subsection{Linear Transformation}
\begin{definition}
    Given two vector spaces $\mathbb{E}$, $\mathbb{V}$, $f: \mathbb{E} \to \mathbb{V}$ is a linear transformation if
    \begin{align*}
        \forall x,y \in \mathbb{E}, \forall \alpha, \beta \in \mathbb{R}:\\
        A(\alpha x + \beta y) = \alpha A(x) + \beta A(y)
    \end{align*}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item All linear transformations from $\mathbb{R}^n \to \mathbb{R}^m$ are of the from
            \begin{align*}
                A(x) = Ax \quad \text{for some matrix } A \in \mathbb{R}^{m \times n}
            \end{align*}
        \item All linear transformations from $\mathbb{R}^{n \times n} \to \mathbb{R}^k$ are of the form:
            \begin{align*}
                A(X) = \begin{bmatrix} \text{trace}(A_{1}^\top X) \\ \text{trace}(A_{2}^\top X) \\ 
                                \vdots \\ \text{trace}(A_{n}^\top X) \end{bmatrix} \quad \forall \; X \in \mathbb{R}^{m \times n}
            \end{align*}
            some matrices $A_{1}, \ldots ,A_{k} \in \mathbb{R}^{m \times n}$
        \item The identity transformation $\mathcal{I}: \mathbb{E} \to \mathbb{E}$ is defined by $\mathcal{I}(x) = x$
    \end{enumerate}
\end{example}

\subsection{The Dual Space}
\begin{definition}
    The dual space of a vector space $\mathbb{E}$ is the space of all linear functionals on $\mathbb{E}$
\end{definition}

For inner product spaces, (Riez Representation) 
for any linear functional $f$, $\exists v \in \mathbb{E}$ s.t $f(x) = \langle \vv,\vx \rangle
    \quad \forall \vx \in \mathbb{E}$. \\
We write $\vv \in \mathbb{E}^*$ (notation). \\
Elements of $\mathbb{E}^*$ and $\mathbb{E}$ are the same if $\mathbb{E}$ we use a norm $\| \cdot \| $,
then in $\mathbb{E}^*$ we use the norm dual to it, defined by (dual norm )
\begin{align*}
    \forall \vy \in \mathbb{E}^*: \| \vy \|_{*} := \max_{\vx \in \mathbb{E}: \| x \| \le 1 } \langle \vy,\vx \rangle
\end{align*}


\begin{theorem}
    Generalized Cauchy-Schwarz:
    \begin{align*}
        \forall \vx \in \mathbb{E}, \forall \vy \in \mathbb{E}^*:
            \| \langle \vx,\vy \rangle \| \le \| \vx \| \| \vy \| _{*}
    \end{align*}
\end{theorem}


\begin{theorem}
    Euclidean norms are self-dual. We say that Euclidean space "self-dual" and write $\mathbb{E} = \mathbb{E}^*$
\end{theorem}

\begin{example}
    \begin{enumerate}
        \item In $\mathbb{R}^d$, with $\langle \vx, \vy \rangle = \vx^\top \vy$
            \begin{enumerate}
                \item The norm dual to $l_{p}$ norm for $p > 1$ is the norm $l_{p}^*$ where 
                    $\frac{1}{p} + \frac{1}{p^*} = 1$. $l_1$ and $l_{\infty}$ are dual to each other.
                \item The norm dual to $\| \cdot \|_{Q}$ for $Q$ symmetric, positive definite is 
                        $\| \cdot \|_{Q^{-1}}$
                        \begin{align*}
                            \| \vx \|_{Q^{-1}} = \Big(\vx^\top Q^{-1} x \Big)^{\frac{1}{2}}
                        \end{align*}
                        If $Q = \text{diag}(w_{1}, \ldots ,w_{d})$ for positive $w_{1}, \ldots ,w_{d}$,
                        then $\| \vx \|_{Q^{-1}} = \Big(\sum_{i=1}^{d} \frac{1}{w_{i}}\vx_{i}^2\Big)^{\frac{1}{2}}$
            \end{enumerate}
        \item $E = E_1 \times \cdots \times E_m$, with $\|  \cdot \|_{E_1}, \ldots , \|  \cdot \| _{E_{m}}$
            \begin{align*}
                \| (\vv_{1}, \ldots ,\vv_{m}) \|_{\mathbb{E}} = \Big(\sum_{i=1}^{m} w_{i} \| \vv_{i} \|_{\mathbb{E}_{i}}^2 \Big)^{\frac{1}{2}}\\
                \| (\vw_{1}, \ldots ,\vw_{m}) \|_{\mathbb{E}^*} = \Big(\sum_{i=1}^{m} \frac{1}{w_{i}} \| \vu_{i} \|_{\mathbb{E}_{i}^*}^2 \Big)^{\frac{1}{2}}
            \end{align*}
    \end{enumerate}
\end{example}

\begin{theorem}
    Bidual space = dual space to $\mathbb{E}^*$. \\
    In finite vector space, $\mathbb{E}^{**} = \mathbb{E}$
\end{theorem}

\begin{theorem}
    $\langle A\vx,\vy \rangle \le \| A \|_{a,b} \| \vx \|_{a} \| \vy \|_{b}$ if $\| \cdot  \|_{a} $
        and $\| \cdot  \| _{b}$ are dual to each other.
\end{theorem}


\subsection{Adjoint Transformation}
\begin{definition}
    Given vector space $\mathbb{E}$ and $\mathbb{V}$, and a linear transformation $A: \mathbb{E} \to \mathbb{V}$,
        the adjoint transformation $A^\top: \mathbb{V}^* \to \mathbb{E}^*$ is defined by
        \begin{align*}
            \langle \vy, A(x) \rangle = \langle A^\top(y), \vx \rangle
        \end{align*}
\end{definition}


\begin{example}
    In particular,
    \begin{enumerate}
        \item If $\mathbb{E} = \mathbb{R}^n, \mathbb{V} = \mathbb{R}^m$, $\langle \vx,\vy \rangle = \vx^\top \vy$,
            then, $A(x) = Ax$ for some $A \in \mathbb{R}^{m \times n}$ and $A^\top (y) = A^\top \vy$
        \item $\mathbb{E} = \mathbb{R}^{m \times n}, \mathbb{V} = \mathbb{R}^k$
    \end{enumerate}
\end{example}

\DATE{Sep 13, 2024}
Given $A: \mathbb{E} \to \mathbb{V}$, $\| \cdot  \|_{\mathbb{E}}, \| \cdot  \|_{\mathbb{E}} $, we define the norm
$\| A \| =\sup_{x\in \mathbb{E}, \| x \|_{\mathbb{E}} \le 1 } \| A(x) \|_{\mathbb{V}} $


\section{Extended Real-Valued Functions}
\begin{definition}
    functions that map some real vector space $(\mathbb{E}, \langle \cdot ,\cdot  \rangle), \| \cdot  \| $ to
    the extended real line -either $\mathbb{R} \bigcup \{ -\infty,+\infty \} \equiv [-\infty,+\infty]$ or 
    $\mathbb{R} \bigcup \{ +\infty \} \equiv (-\infty,+\infty]$
\end{definition}

\begin{align*}
    \begin{aligned}
        \min_{x \in \mathbb{E}} &\quad f(x)
    \end{aligned}
\end{align*}

Consider this problem, why do we even want to include $+\infty$
\begin{enumerate}
    \item $f$ is not everywhere defined  on $\mathbb{E}$, I can assign it to $+\infty$ at points where it's not
    defined. So when it becomes well-defined on all $\mathbb{E}$.
    
    Here we define the domain $=$ effective domain:
    \begin{align*}
        dom(f) = \{ x \in \mathbb{E}: f(x) < +\infty \}
    \end{align*}
    \item We can think of all optimization problems whether constrained or unconstrained, as unconstrained optimization
    problem.
    \begin{align*}
        \begin{aligned}
            \min_{x\in \mathcal{X}}  f(x) \iff \min_{x \in \mathbb{E}} f(x) + \delta_{\mathcal{X}}(x)\\
            \text{where } \delta(x) = 
            \begin{cases} 
            0, &  for x \in \mathcal{X}\\ 
            +\infty, &  o.w. 
            \end{cases}
        \end{aligned}
    \end{align*}
\end{enumerate}

"Rules" for dealing with $\pm \infty$ and $a \in \mathbb{R}$:
\begin{enumerate}
    \item $a+\infty = +\infty + a = +\infty$
    \item $a-\infty = -\infty + a = -\infty$
    \item 
    \begin{align*}
        a \cdot \infty = 
        \begin{cases} 
        \infty, &if \;a > 0  \\ 
        -\infty, &if \;a < 0
        \end{cases}
    \end{align*}
    \item $0 \cdot \pm \infty = 0$
    \item $-\infty < a < \infty \quad \forall a \in \mathbb{R}$
\end{enumerate}

\subsection{Closed Functions}
\begin{definition}
    $epi(f) := \{ (x,y): x \in \mathbb{E}, y \in \mathbb{R}, f(x) \le y \}$
\end{definition}

\begin{definition}
    A function $f: \mathbb{E} \to [-\infty,\infty]$ is said to be closed if $epi(f)$ is closed.
\end{definition}

\begin{proposition}
    For $C \subseteq \mathbb{E}$, $\sigma_{C}(x)$ is closed $\iff C$ is closed.
\end{proposition}
\begin{proof}
    $epi(C) = C \times \mathbb{R}_{+}$
\end{proof}

\begin{remark}
    $f$ is closed $\not\iff$ $dom(f)$ is closed.
\end{remark}

\begin{example}
    \begin{align*}
        f(x) = \begin{cases} 
        \frac{1}{x}, &x > 0  \\ 
        \infty, & x \le 0   
        \end{cases}
    \end{align*}
    Then $dom(f) = (0,\infty)$ is open. And we see that:
    \begin{align*}
        epi(f) = \{ (x,y) \in \mathbb{R}^{2}: x>0, \frac{1}{x} \le y \}
    \end{align*}
\end{example}

\subsubsection{Related Concepts}
\begin{enumerate}
    \item Lower Semicontinuity:
    \begin{definition}
        $f:\mathbb{E} \to [-\infty,+\infty]$ is l.s.c. at $x \in \mathbb{E}$ if 
        \begin{align*}
            f(x) \le \liminf_{n \to \infty} f(x_n) 
        \end{align*}
        for any sequence $\{ x_n  \}_{n\ge 1} \in \mathbb{E}$ s.t.
        $x_n \to x $ as $n \to \infty$.

        f is said to be l.s.c. if it is l.s.c. at all $x \in \mathbb{E}$.
    \end{definition}
    \item Level set: 
    defined for $\alpha \in \mathbb{R}, \; f:\mathbb{E} \to [-\infty,+\infty]$.
    \begin{align*}
        Lev(f,\alpha) = \{ x \in \mathbb{E}:f(x) \le \alpha \}
    \end{align*}
\end{enumerate}

\begin{theorem}
    If $f:\mathbb{E} \to [-\infty,+\infty]$. Then all of the following statements are equivalent:
    \begin{enumerate}
        \item $f$ is l.s.c.
        \item $f$ is closed.
        \item $Lev(f,\alpha)$ is closed, $\forall \alpha \in \mathbb{R}$
    \end{enumerate}
\end{theorem}

\subsubsection{Operations preserving closedness}
\begin{enumerate}
    \item If $f: \mathbb{V} \to [-\infty,+\infty]$ is closed, $A:\mathbb{E} \to \mathbb{V}$ is a linear transformation
    and $b \in \mathbb{V}$, then 
    \begin{align*}
        g(x) = f(A(x)+b) \text{ is closed.}
    \end{align*}
    \item If $f_1, \ldots ,f_m: \mathbb{E} \to (-\infty,+\infty]$ are closed and $\alpha_1, \ldots ,\alpha_m \in \mathbb{R}_{+}$,
    then 
    \begin{align*}
        f(x) = \sum_{i=1}^{n}\alpha_{i} f_{i}(x) \text{ is closed}
    \end{align*}
    \item Given an index set $I$ and functions $f_i: \mathbb{E} \to (-\infty,\infty]$, $i \in I$,
    that are closed, the function 
    \begin{align*}
        f(x) = \sup_{i \in I} f_{i}(x) \text{ is closed.}
    \end{align*}
\end{enumerate}

\subsubsection{Closedness vs Continuity}
Bottom line: If $f$ has closed domain + continuous over the domain $\Longrightarrow$ closed.

But closed $\not\iff$ continuous over the domain.

\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,+\infty]$ be continuous over its domain and suppose $dom(f)$ is closed
    $\Longrightarrow$ f is closed.
\end{theorem}

\begin{proof}
    Argue that $epi(f)$ is closed.

    Take any sequence  $\{ (x_n,y_n) \}_{n\ge 1} \in epi(f)$ that converges to some $(x_{*}, y_{*})$ as $n \longrightarrow \infty$

    To argue: $(x_{*}, y_{*}) \in epi(f)$: we know that $x_n \in dom(f)$, $x_n \longrightarrow x_{*}$,
    $dom(f)$ is closed $\Longrightarrow x_{*} \in dom(f)$

    By the definition of $epi(f)$:
    \begin{align*}
        f(x_n) \le y_n
    \end{align*}
    Since f is continuous over $dom(f)$ and $\{ x_n \}_{n}, x_{*} \in dom(f)$ we can take the limit $n \longrightarrow \infty$
    \begin{align*}
        f(x_{*}) \le y_{*}\\
        \Longrightarrow (x_{*},y_{*}) \in epi(f)
    \end{align*}
\end{proof}

\begin{example}[closed $\not\Longrightarrow$ continuous on its domain]
 \item 
 \begin{align}
    f_{\alpha}(x) = \begin{cases} 
    \alpha, &  x=0\\ 
    x, &   0 < x \le 1\\
    \infty, & elsewhere
    \end{cases}
 \end{align}
 When $\alpha < 0$, then it's l.s.c., i.e., closed, but it's not continuous.
 \item $l_{0}$ "norm"
 \begin{align*}
    f(x) = \| \vx \| _{0} = |\{ i:\vx_{i} \neq 0 \}| 
 \end{align*}
 $f$ is not continuous but it's closed.

 \begin{align*}
    f(x) = \sum_{i=1}^{d}I(\vx_{i})
 \end{align*}
 where 
 \begin{align*}
    I(y) = \begin{cases} 
    0, & y=0 \\ 
    1, & y\neq 0  
    \end{cases}
 \end{align*}
 We know 
 \begin{align*}
    Lev(I,\alpha) = \begin{cases} 
    \emptyset , &  \alpha < 0\\ 
    \{ 0 \}, &   0 \le \alpha < 1\\
    \mathbb{R}, & \alpha \ge 1
    \end{cases}
 \end{align*}
 Then $I$ is closed. $\Longrightarrow$ the sum of them is closed.
\end{example}


\DATE{Sep 16, 2024}
\begin{theorem}[Weierstrass theorem for closed functions]
    Let $f: \mathbb{E}\to (-\infty,\infty]$ be a $\underbrace{\text{proper}}_{dom(f)\neq \emptyset}$,
    closed function and let $C \subseteq \mathbb{E}$ be a compact set such that 
    $C \bigcap dom(f)\neq \emptyset$. Then:
    \begin{enumerate}
        \item $f$ is bounded below on $C$.
        \item $f$ attains its minimal value over $C$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item Suppose for the purpose of contradiction (FPOC) that $f$ is not bounded below on $C$.
        Then $\exists $a sequence $\{ x_n \}_{n\ge 1}$, $x_n \in C \forall n$, s.t.
        \begin{align*}
            \lim_{n \to \infty}f(x_n) = -\infty
        \end{align*}
        By Bolzano-Weierstrass, since $C$ is compact, there exists a subsequence $\{ x_{n_k} \}_{k\ge 1}$
        that converges to a point $\bar{x} \in C$. Since 
        \begin{align*}
            f \text{ is closed } \iff f \text{ is l.s.c.}
        \end{align*}
        We know 
        \begin{align*}
            f(\bar{x}) \le \lim_{k \to \infty}f(x_{n_k}) = -\infty\\
            \Longrightarrow f(\bar{x}) = -\infty
        \end{align*}
        Contradiction.
        \item Let $f_{*} = \inf_{x \in C} f(x) > -\infty$.
        \begin{claim}
            $\exists $ a sequence $\{ x_n    \}_{n \ge 1}$ s.t.
            \begin{align*}
                f(x_n) \rightarrow f_{*} \text{ as } n\rightarrow\infty
            \end{align*}
        \end{claim}
        Then $(x_n, f(x_n)) \in epi(f)$. Then take a subsequence $\{ x_{n_{k}} \}_{k\ge 1}$ s.t.
        $x_{n_{k}} \rightarrow \bar{x}$. Then
        \begin{align*}
            f(\bar{x}) \le \lim\inf_{k \rightarrow\infty}f(_{n_{k}}) = f_{*}\\
            \Longrightarrow \bar{x} \text{ minimizes } f
        \end{align*}
    \end{enumerate}
\end{proof}

What is we are not optimizing over a compact set.
\begin{definition}
    A proper function $f: \mathbb{E} \to (-\infty,\infty]$ is said to be coercive if 
    \begin{align*}
        \lim_{x \in \mathbb{E}:\| x \|  \to \infty} f(x) = +\infty
    \end{align*}
\end{definition}

\begin{theorem}
    Let $f:\mathbb{E} \to (-\infty,\infty]$ be a proper, closed and coercive function, and let $S
    \subseteq \mathbb{E}$ be a nonempty closed set that satisfy $S \bigcap dom(f) \neq \emptyset$.
    Then $f$ attains the minimum over set $S$.
\end{theorem}
\begin{proof}
    Let $x_0$ be an arbitrary point
\end{proof}

\subsection{Convex Function}
\begin{definition}[Equivalent definitions of convexity]
    f is convex if 
    \begin{enumerate}
        \item $epi(f)$ is convex
        \item $\forall x,y \in \mathbb{E}, \forall \alpha \in (0,1)$:
        \begin{align*}
            f((1-\alpha)x + \alpha y ) \le (1-\alpha)f(x) + \alpha f(y)
        \end{align*}
        \begin{remark}
            Notice this induces Jensen's inequality: $\forall x_1, \ldots ,x_m \in \mathbb{E},
            \forall \lambda_1, \ldots ,\lambda_m \ge 0, \sum_{i=1}^{n}\lambda_i = 1$
            \begin{align*}
                f(\sum_{i=1}^{m}\lambda_i x_i) \le \sum_{i=1}^{m} \lambda_i f(x_i)
            \end{align*}
        \end{remark}
        \item if $f$ is continuously differentiable: $\forall x,y \in \mathbb{E}$
        \begin{align*}
            f(y) \ge f(x) + \langle \nabla f(x),y-x \rangle
        \end{align*}
        \item if $f \in C^{2}$: $\forall x \in \mathbb{E}$:
        \begin{align*}
            \nabla^{2}f(x) \succcurlyeq 0
        \end{align*}
    \end{enumerate}
\end{definition}

\begin{theorem}[Operations preserving convexity]
    \begin{enumerate}
        \item If $A: \mathbb{E} \to \mathbb{V}$ liner transform, $b \in \mathbb{V}$,and $f: \mathbb{V} \to { (-\infty,\infty]}$ is convex, then $f(A(x)+b)$ is convex.
        \item $f_1, \ldots ,f_{m}: \mathbb{E} \to (-\infty,+ \infty]$ are convex, $\lambda_1, \ldots ,
        \lambda_n \in \mathbb{R}_{+}$, then $f(x) = \sum_{i=1}^{m} \lambda_i f_i(x)$ is convex.
        \item $I:$ inded set, $f_i: \mathbb{E} \to (-\infty,\infty]$ convex $\forall i \in I$,
        then $f(x) = \sup_{i \in I}f_i(x)$ is convex.
    \end{enumerate}
\end{theorem}

\begin{example}
    Given $C \subseteq  \mathbb{E}$ that is nonempty (but not necessarily convex), let
    \begin{align*}
        d_{C}(x) = \inf_{y \in C}\| y-x \| 
    \end{align*}
    If $\mathbb{E}$ is Euclidean, then $\varphi_{C}(x) = \frac{1}{2}(\| x \|^{2} - 
    d_{C}^{2}(x) )$ is convex.
    Notice that 
    \begin{align*}
        d_{C}^{2}(x) = \inf_{y \in C}\| y-x \|^{2} = \inf_{y \in C} \{ \| x \|^{2} - 2
        \langle x,y \rangle +\| y \|^{2}  \}\\
        = \| x \| ^{2} -\sup_{y \in C} \{ 2\langle y,x \rangle -\| y \|^{2} \}
    \end{align*}
\end{example}

\begin{theorem}[Convexity under partial minimization] \label{thm:2.8}
    Let $f: \mathbb{E}\times \mathbb{V} \to (-\infty,\infty]$ be a convex function s.t.
    $\forall  x \in \mathbb{E}, \,\exists y \in \mathbb{V}: \; f(x,y) < \infty$.
    Let $g: \mathbb{E} \to [-\infty,\infty)$ be defined 
    \begin{align*}
        g(x) := \inf_{y \in \mathbb{V}}f(x,y)
    \end{align*}
    Then $g$ is convex.
\end{theorem}
\begin{proof}
    To show $\forall x_1,x_2 \in \mathbb{E}, \forall \alpha \in (0,1)$:
    \begin{align*}
        g((1-\alpha)x_1 + \alpha x_2) \le (1-\alpha)g(x_1) + \alpha g(x_2)
    \end{align*}
    \begin{enumerate}
        \item[Case 1:] $g(x_1),g(x_2) > -\infty$. Take any $\epsilon > 0$, then $\exists \,
        y_1,y_2 \in \mathbb{E}$ s.t.
        \begin{align*}
            f(x_1,y_1) \le g(x_1) + \epsilon\\
            f(x_2,y_2) \le g(x_2) + \epsilon
        \end{align*}
        $f$ is convex so:
        \begin{align*}
            f((1-\alpha)x_1+\alpha x_2, (1-\alpha)y_1 + \alpha y_2) \le (1-\alpha)f(x_1,y_1) + \alpha
            f(x_2,y_2) \\
            \le (1-\alpha)g(x_1) + \alpha g(x_2) + \epsilon
        \end{align*}
        Then by the definition of $g$, we have:
        \begin{align*}
            g((1-\alpha)x_1 + \alpha x_2) \le (1-\alpha)g(x_1) + \alpha g(x_2) + \epsilon
            \quad \forall \epsilon > 0
        \end{align*}
        \item[Case 2:] Assume at least one of $g(x_1), g(x_2)$ is equal $-\infty$. Want to show:
        \begin{align*}
            g((1-\alpha)x_1 + \alpha x_2) = -\infty
        \end{align*}
        Take any $M \in \mathbb{R}$, then $\exists y_1 $ s.t. $f(x_1,y_1) \le M$.
        And $\exists y_2$ s.t. $f(x_2,y_2) < \infty$. Since f is convex
        \begin{align*}
            f((1-\alpha)x_1 + \alpha x_2, (1-\alpha)y_1, \alpha y_2) \\
            \le (1-\alpha)f(x_1,y_1) + \alpha f(x_2,y_2) \\
            \le (1-\alpha)M + \alpha f(x_2,y_2)
        \end{align*}
        Then by the definition of $g$, we have
        \begin{align*}
            g((1-\alpha)x_1 + \alpha x_2) \le (1-\alpha)M + \alpha f(x_2,y_2)
        \end{align*}
        M is arbitrary.
    \end{enumerate}
\end{proof}

\subsubsection{Infimal Convolution}
\begin{definition}
    $h_1,h_2: \mathbb{E} \to (-\infty,\infty]$, both proper
    \begin{align*}
        h_1 \square h_2(x) = \inf_{u \in \mathbb{E}}\{ h_1(u)+h_2(x-u) \}
    \end{align*}
\end{definition}
\begin{remark}
    It's important for proximal point method. You smoothe functions by infimal convolution with 
    some good functions like quadratic functions.
\end{remark}

\vspace{15mm}
\DATE{Sep 20, 2024}
\vspace{15mm}

\begin{theorem}
    Let $h_1: \mathbb{E} \to (-\infty,\infty]$ be proper and convex, $h_2: \mathbb{E}
    \to \mathbb{R}$ be a real-valued convex function. Then $h_1 \square h_2$ is convex.
\end{theorem}
\begin{proof}
    Define $f(x,y) = h_1(y) + h_2(x-y)$, $g(x) = \inf_{y\in \mathbb{E}}f(x,y) = 
    (h_1 \square h_2)(x)$

    Notice that $f$ is convex since it's the sum of two convex functions, and $h_2: \mathbb{E} \times 
    \mathbb{E} \to \mathbb{R}$ is convex since $x-y$ is a linear transform of $x,y$.

    Want to show that $\forall \vx \in \mathbb{E},\, \exists \vy \in \mathbb{E}$ s.t.
    \begin{equation}
        h_1(y) + h_2(x-y) < \infty
    \end{equation}
    It's obvious since $h_1$ is proper and $h_2$ is real-valued.

    Then by Theorem~\ref{thm:2.8}, $f$ is convex.
\end{proof}

\begin{example}
    \item If $C \subseteq \mathbb{E} \neq \emptyset$ is convex, then 
    \begin{align*}
        d_{C}(x) = \inf_{y \in C} \| \vy - \vx \| 
    \end{align*}
    is convex. (This holds for any norm.) 
    \begin{remark}
        $\| \cdot  \| $ is convex.
    \end{remark}
    We write 
    \begin{align*}
        d_{C}(x) = \inf_{\vy \in \mathbb{E}}\{ \| \vy-\vx \|  + \delta_{C}(y)\}\\
        = \underbrace{\delta_{C}}_{\text{convex  \& proper}}  \square \underbrace{\| \cdot  \| }_{\text{convex \& 
        real-valued}}
    \end{align*}
    Notice that
    $\delta_{C}(\cdot )$ is convex when $C$ is a convex set.
\end{example}

\subsubsection{Continuity of convex functions}
\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be convex. let $x_0 \in intdom(f)$. Then
    $\exists \epsilon > 0$ and $L > 0$ s.t. 
    $\underbrace{B[x_0,\epsilon]}_{\text{closed ball, centered at }x_0 \text{ of radius }\epsilon}
    \subseteq dom(f)$ and 
    \begin{align*}
        \forall x \in B[x_0,\epsilon]: |f(x) - f(x_0)| \le L\| \vx_0 - \vx_{0} \| 
    \end{align*}
\end{theorem}
% \begin{remark}
%     In complexity of minimization, what problems are solvable? When we talk about complexity,
%     we take some compact set, how many query we need? The results depend on the global $L$ are classical.
% \end{remark}

\subsection{Support Function}
\begin{definition}
    Let $C \subseteq \mathbb{E}$ be nonempty. Then the support function of $C$ is defined by
    \begin{align*}
        \sigma_{C}: \mathbb{E}^{*} \to (-\infty,\infty]\\
        \sigma_{C}(y) = \sup_{\vx \in C} \langle \vx,\vy \rangle \\
        \text{Or: } \sigma_{C}(y) = \sup_{\vx \in \mathbb{E}}\{ \langle \vx,\vy \rangle 
        - \delta_{C}(x)\}
    \end{align*}
\end{definition}

\begin{lemma}
    Let $C \subseteq \mathbb{E}$ be a nonempty set. Then $\sigma_{C}$ is both closed and convex.
\end{lemma}

\subsubsection{Operations on sets}
\begin{enumerate}
    \item Minkowski sum:
    \begin{align*}
        A, \,B \subseteq \mathbb{E}, A+B = \{ a+b: a\in A, b\in B \}
    \end{align*}
    \item for $\alpha \in \mathbb{R}$, $A \subseteq \mathbb{E}$:
    \begin{align*}
        \alpha A = \{ \alpha a: a\in A \}
    \end{align*}
\end{enumerate}

\begin{proposition}[Properties of support functions:]
    \begin{enumerate}
        \item positive homogeneity:
        \begin{align*}
            \forall C \subseteq \mathbb{E}, C \neq \emptyset, \forall \vy \in \mathbb{E}^{*},\,
            \forall  \alpha \ge 0:\\
            \sigma_{C}(\alpha y) = \alpha \sigma_{C}(y)\\
            \sigma_{\alpha C}(\vy) = \alpha \sigma_{C}(\vy)
        \end{align*}
        \item subadditivity: $\forall C \subseteq \mathbb{E}, \, C \neq \emptyset$,
        \begin{align*}
            \forall vy_1,\,vy_2 \in \mathbb{E}^{*}: \sigma_{C}(vy_1+vy_2) \le \sigma_{C}(\vy_1) +
            \sigma_{C}(\vy_2)
        \end{align*}
        \item $\forall A,\,B \subseteq  \mathbb{E}$, $A \bigcup B \neq \emptyset$, $\forall \vy \in
        \mathbb{E}^{*}:$
        \begin{align*}
            \sigma_{A+B}(\vy) = \sigma_{A}(\vy) + \sigma_{B}(\vy)
        \end{align*}
    \end{enumerate}
\end{proposition}

\begin{example}
    \begin{enumerate}
        \item $C = conv\{ \vb_1,, \ldots ,\vb_m \}, \; \vb_i \in \mathbb{E} \quad \forall\, i$, then
        \begin{align*}
            \sigma_{C}(\vy) = \max_{1 \le i \le m} \langle \vb_i,\vy \rangle
        \end{align*}
        \item Let $K \subseteq \mathbb{E}$ be a cone set s.t. if $\vx \in K$, then $\forall r > 0,
        \; r\vx \in K$.

        The polar cone of $K$ is defined by:
        \begin{align*}
            K^{o} := \{ \vy \in \mathbb{E}^{*}: \langle \vy,\vx \rangle \le 0, \; \forall \,\vx \in K \}
        \end{align*}
        Then $\sigma_{K}(\vy) = \delta_{K^{o}}(y)$

        \item $\mathbb{E} = \mathbb{R}^{d}$, $C = \mathbb{R}_{+}^{d}$
        \begin{align*}
            \sigma_{\mathbb{R}_{+}^{d}}(\vy) = \delta_{\mathbb{R}_{-}^{d}}(\vy)
        \end{align*}

        \item $\mathbb{E} = \mathbb{R}^{d}, \,A \in \mathbb{R}^{n\times d}$, $
        S = \{ \vx \in \mathbb{R}^{d}: \, Ax \le 0 \}$
        \begin{align*}
            \sigma_{S}(\vy) = \delta_{S^{o}}(\vy) \\
            \text{where } S^{o} = \{ A^{\top}\lambda: \lambda \in \mathbb{R}_{+}^{n} \}
        \end{align*}
        \item $C = B_{\| \cdot  \| }[0,1] = \{ \vx \in \mathbb{E}: \| \vx \| \le 1 \}$
        \begin{align*}
            \sigma_{C}(\vy) = \sup_{\vx \in \mathbb{E}: \| \vx \| \le 1}\langle \vy,\vx \rangle 
            = \| \vy \|_{*}
        \end{align*}
    \end{enumerate}
\end{example}

\begin{proposition}
    If $A \subseteq  \mathbb{E}, \, A \neq \emptyset,$ then
    \begin{enumerate}
        \item $\sigma_{A} = \sigma_{cl}(A)$ where $cl(A) = $ closure of $A$
        \item $\sigma_{A} = \sigma_{conv}(A)$ where $conv(A) = $ convex hull of $A$
    \end{enumerate}
    If $A,\, B \subseteq \mathbb{E}$ are closed, convex and nonempty, then 
    \begin{align*}
        A = B \iff \sigma_{A} = \sigma_{B}
    \end{align*}
\end{proposition}

\newpage
\section{Subdifferentiation}
\begin{definition}
    The directional derivative of a function $f: \mathbb{E} \to [-\infty,\infty]$ at $\bar{\vx} \in \mathbb{E}$
    in a direction $\vz \in \mathbb{E}$ is:
    \begin{align*}
        f'(\bar{\vx}; \vz) = \lim_{\alpha \to 0} \frac{f(\bar{\vx}+\alpha \vz) - f(\bar{\vx})}{\alpha}
    \end{align*}
    when this limit exists.

    When the directional derivative $f'(\bar{\vx}; \vz)$ is linear in $\vz$, then we say that 
    $f$ is Gateaux differentiable.

    $\mathbb{E} = \mathbb{R}^{d}$. $\exists g \in \mathbb{E}^{*}$ s.t. $f'(\vx,\vz) = \langle 
        g, \vz
    \rangle$, we say $g$ is the Gateaux derivative.

    If $f$ is differentiable on every point of $C \subseteq \mathbb{E}$, we say that $f$ is differentiable on
    $C$.
\end{definition}

\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be a proper convex function and let $\vx \in intdom(f)$.
    Then $\forall \vz in \mathbb{E}$, the directional derivative $f'(\vx;\vz)$ exists.

\end{theorem}
\begin{exercise}
    Show that if $f$ attains $-\infty$, then it would be $-\infty$ anywhere.
\end{exercise}

\begin{lemma}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be a proper convex function and let $\vx \in intdom(f)$. Then
    \begin{enumerate}
        \item $\vz \mapsto f'(\vx;\vz)$ is convex;
        \item $\forall \lambda > 0, \forall \vz \in \mathbb{E}: f'(\vx;\lambda \vz) = \lambda f'(\vx;\vz)$
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item Take $\vz_1;\vz_2 \in \mathbb{E}$ and $\lambda \in (0,1)$.
        \begin{align*}
            f'(\vx;\lambda \vz + (1-\lambda)) = \lim_{\alpha \to 0} \frac{
                f(\vx + \alpha (\lambda \vz_1 + (1-\lambda)\vz_2) - f(\vx))
            }{\alpha}\\
            = \lim_{\alpha \to 0} \frac{f(\lambda(\vx + \alpha \vz_1) + (1-\lambda)(\vx + \alpha \vz_2))
            - f(\vx)}{\alpha}\\
            \le \lambda \lim_{\alpha \to 0} \frac{f(\vx + \alpha \vz_1 ) - f(x)}{\alpha} 
            + (1-\lambda) \lim_{\alpha \to 0} \frac{f(\vx + \alpha \vz_2 ) - f(x)}{\alpha} \\
            = \lambda f'(\vx; \vz_1) + (1-\lambda)f'(\vx;\vz_2)
        \end{align*}
        \item $\lambda = 0$ Trivial. Assume $\lambda > 0$:
        \begin{align*}
            f'(\vx;\lambda\vz) = \lambda 
            \lim_{\alpha \to 0} \frac{f(\bar{\vx}+\lambda\alpha \vz) - f(\bar{\vx})}{\lambda\alpha}
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{lemma}
    Let $f: \mathbb{E} \to (-\infty, \infty]$ be a proper convex function and let
    $\vx \in intdom(f)$. Then:
    \begin{align*}
        f(\vy) \ge f(\vx) + f'(\vx;\vy - \vx) \quad \forall\; \vy \in dom(f)
    \end{align*}
\end{lemma}

\begin{proof}
    \begin{align*}
        f'(\vx; \vy - \vx) &= \lim_{\alpha \to 0} \frac{f((1-\alpha)\vx + \alpha\vy) - f(\vx)}{\alpha}\\
    &\le f(\vy) - f(\vx)
    \end{align*}
\end{proof}

\subsection{Directional derivative of a max-type function}

\begin{theorem}
    Suppose that $f(\vx) = \max_{1\le i \le m} f_i(\vx)$ where $f_1, \ldots ,f_m:
    (-\infty,\infty]$ are proper. Let $\vx \in \bigcap_{i=1}^{m} intdom(f_i)$ and let $\vz \in \mathbb{E}$.
    Assume that $f_i'(\vx;\vz)$ exists, $\forall \, i \in \{ 1, \ldots ,m \}$. Then:
    \begin{align*}
        f'(\vx;\vz) = \max_{i \in I(\vx)}f_i'(\vx;\vz),
    \end{align*}
    where $I(\vx) = \{ i: s.t \; f_i(\vx) = f(\vx) \}$
\end{theorem}
\begin{proof}
    For any $i \in \{ 1, \ldots ,m \}$:
    \begin{align*}
        \lim_{\alpha \to 0^{+}}f_i(\vx + \alpha \vz) = \lim_{\alpha \to 0}
        \Big\{ \alpha \frac{f_i(\vx + \alpha \vz) - f_i(\vx)}{\alpha} + f_i(\vx) \Big\}\\
        = 0 \cdot f_i'(\vx; \vz) + f_i(\vx)\\
        = f_i(\vx)
    \end{align*}
    By the definition of $I(\vx)$, $f_i(\vx) > f_j(\vx), \; \forall i \in I(\vx), \, j\neq I(\vx)$,

    $\Longrightarrow \exists \epsilon > 0,\; \forall \,\alpha \in (0,\epsilon]$ s.t. 
    \begin{align*}
        f_i(\vx + \alpha\vz) > f_j(\vx + \alpha\vz) \quad \forall  i \in I(\vx), j \notin I(\vx)\\
        \Longrightarrow \forall  \, \alpha \in (0,\epsilon]:
        \; f(\vx + \alpha\vz) = \max_{i \le i \le m} f_i(\vx + \alpha\vz) \\
        = \max_{i \in I(\vx)}f_i(\vx + \alpha\vz)\\
        \Longrightarrow\forall \alpha \in (0,\epsilon]:\\
        \frac{f(\vx + \alpha\vz) - f(\vx)}{\alpha} = 
        \frac{\max_{i \in I(\vx)}(f_i(\vx+\alpha\vz) - f_i(\vx))}{\alpha}
    \end{align*}
    We obtain:
    \begin{align*}
        \lim_{\alpha \to 0}\frac{f(\vx + \alpha\vz) - f(\vx)}{\alpha} &= 
        \lim_{\alpha \to 0}\max_{i \in I(\vx)}\frac{f_i(\vx + \alpha\vz) - f_i(\vx)}{\alpha}\\
        &= \max_{i \in I(\vx)}\lim_{\alpha \to 0}\frac{f_i(\vx + \alpha\vz) - f_i(\vx)}{\alpha} \\
        &= \max_{i \in I(\vx)}f_i'(\vx; \vz)
    \end{align*}
\end{proof}

\subsection{Subgradient}
\begin{definition}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be a proper function and let $\vx \in dom(f)$. A
    vector $g \in \mathbb{E}^{*}$ is a subgradient of $f$ at $x$ if 
    \begin{align*}
        \forall \, \vy \in \mathbb{E}: \; \underbrace{f(\vy) \ge f(\vx) + \langle g, \vy-\vx \rangle}
        _{\text{subgradient inequality}}
    \end{align*}
\end{definition}

The set of all subgradient of $f$ at $\vx$ is called the subdifferential of $f$ at $\vx$
and denoted by $\partial f(\vx)$. If $\partial f(\vx) \neq \emptyset$,
 we say that $f$ is subdifferentiable at 
$\vx$.
\begin{align*}
    \partial f(\vx) = \{ g \in \mathbb{E}^{*}: f(\vy) \ge f(\vx) + \langle g,\vy-\vx \rangle 
    , \quad \forall \, \vy \in \mathbb{E}\}
\end{align*}
\begin{example}
    \begin{enumerate}
        \item Let $f: \mathbb{E} \to \mathbb{R}$ be defined by 
        \begin{align*}
            f(\vx) = ||\vx||, \quad \text{ where }||\cdot || \text{ is the norm at }\mathbb{E}
        \end{align*}
        Then:
        \begin{align*}
            \partial f(\vec{0}) = B_{\|\cdot \|_{*}}[0,1] = \{ 
                g \in \mathbb{E}^{*}: \| g \| _{*} \le 1
             \}
        \end{align*}
        \begin{proof}
            By the def of a subgradient and subdifferential, $g \in \partial f(\vec{0})$ if and only if 
            \begin{align*}
                \forall \vy &\in \mathbb{E}: \; f(\vy) \ge f(\vec{0}) + \langle g,\vy \rangle\\
                &\iff \langle g,\vy \rangle \le \| \vy \| 
            \end{align*}
            ($\Longrightarrow$) want to show: $\| g \| _{*} \le 1 \Longrightarrow 
            \langle g,\vy \rangle \le \| \vy \| $
            Cauchy-Schwarz

            ($\Longleftarrow$) want to show: $\| g \| _{*} \le 1 \Longleftarrow 
            \langle g,\vy \rangle \le \| \vy \| $
            Definition of dual norm
        \end{proof}

        \DATE{Sep 27, 2024}

        \item  $C \subseteq  \mathbb{E},\, C \neq \emptyset$
        \begin{equation}
            \delta_{C}(x) = 
            \begin{cases} 
            0, & if \vx \in C  \\ 
            +\infty, & o.w.   
            \end{cases}
        \end{equation}
        Then: $\partial \delta_{C}(x) = \underbrace{N_{C}(x)}_{\text{normal cone at } x} \; \forall \vx \in C$,
        where $N_{C}(x) = \{ g \in \mathbb{E}^{*}: \langle g,\vy - \vx \rangle \le 0,\; \forall \vy \in C \}$
        \begin{proof}
            Consider any $\vx \in C$. Then $g \in \partial \delta_{C}(\vx)$ iff 
            \begin{align*}
                \forall \, \vy \in \mathbb{E}: \; \delta_{C}(\vy) \ge \delta_{C}(\vx) + \langle g,\vy - vx \rangle
            \end{align*}
            \begin{equation}
                \begin{cases} 
                \text{if }\vy \notin C, & \delta_{C}(\vy) = +\infty \\ 
                \text{if }\vy \in C: &   \delta_{C}(\vy) = \delta_{C}(\vx) = 0 
                \Longrightarrow \langle g,\vy-\vx \rangle \le 0
                \end{cases}
            \end{equation}
        \end{proof}
        Special case: $C = B_{\| \cdot  \| }[0,1] = \{ \vx \in \mathbb{E}: \, \| \vx \|\le 1  \}$
            \begin{align*}
                \forall  \vx \in C: \partial \delta_{C}(x) = N_{C}(\vx) = \{ 
                    g: \langle g,\vy-\vx \rangle \le 0, \; \forall \vy \in C
                 \}\\
                 \sup_{\vy \in \mathbb{E}: \| \vy \|\le 1 }\langle g, \vy - \vx \rangle 
                 = \| g \|_{*} - \langle g,\vx \rangle \le 0\\
                 \partial \delta_{B\| \cdot  \| }(x) = \{ g \in \mathbb{E}^{*}: 
                 \| g \|_{*} \le \langle g,\vx \rangle \}
            \end{align*}

        \item Subgradient of max eval: $f: \underbrace{\mathbb{S}^{d}}_{\text{the set of all d by d symm matrices}} 
        \to \mathbb{R},\, f(X) := \lambda_{max}(X)$

        Fix $X \in \mathbb{S}^{d}$ and let $\vv$ be a unit eigenvector corresponding to $\lambda_{max}(X)$
        \begin{align*}
            Xv = \lambda_{max}(X)\vv 
        \end{align*}
        Then $\forall Y \in \mathbb{S}^{d}$:
        \begin{align*}
            \lambda_{max}(Y) = \max_{u}\{ \vu^{\top}Y\vu: \| \vu \| \le 1  \} \\
            \ge \vv^{\top}X\vv \\
            = \vv^{\top}X\vv + \vv^{\top}(Y-X)\vv \\
            = \lambda_{max}(X) + \underbrace{Tr(\vv\vv^{\top}(Y-X))}_{\langle \vv \vv^{\top}, Y-X \rangle}\\
            \Longrightarrow \vv \vv^{\top} \in \partial f(\vx)
        \end{align*}
    \end{enumerate}
\end{example}

And for the first example 
\begin{align*}
    -\nabla f(x) \in N_{C}(x) \iff -\nabla f(x) \in \partial \delta_{C}(x) \iff 
    0 \in \nabla f(x) + \partial \delta_{C}(x)
\end{align*}
which corresponds to 
\begin{align*}
    \min_{\vx \in \mathbb{E}}f(x) + \delta_{C}(x) \iff \min_{\vx \in C}f(x)
\end{align*}

\subsubsection{Properties of the subdiff set:}
\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be a proper function. Then 
    $\partial f(x)$ is closed and convex, $\forall \vx \in \mathbb{E}$
\end{theorem}
\begin{proof}
    recall that: Fix $\vx$, define 
    \begin{align*}
        H_y:= \{ g \in \mathbb{E}^{*}: \, f(\vy) \ge f(\vx) + \langle g,\vy-\vx \rangle \}
    \end{align*}
    Notice that $H_y$ is closed and convex 
    \begin{align*}
        \partial f(\vx) = \bigcap_{\vy \in \mathbb{E}}H_y \quad \text{thus closed and convex.}
    \end{align*}
\end{proof}

\begin{lemma}
    Let $f: \mathbb{E} \to (-\infty,+\infty]$ be a proper function and assume that $dom(f)$ is convex.
    Suppose that for any $\vx \in dom(f), \, \partial f(\vx) \neq \emptyset$. Then $f$ is convex.
\end{lemma}
\begin{proof}
    Let $\vx, \vy \in dom(f), \, \alpha \in (0,1)$.
    \begin{align*}
        dom(f) \text{ is convex} \Longrightarrow z := (1-\alpha)\vx + \alpha \vy \in dom(f)\\
        \Longrightarrow \exists g \in \partial f(\vz)
    \end{align*}
    since $\partial f(\vz) \neq \emptyset$

    Since $g$ is a subgradient at $\vz$:
    \begin{equation}
        \begin{cases} 
        f(\vx) \ge f(\vz) + \langle g, \vx -\vz \rangle, &  \\ 
        f(\vy) \ge f(\vz) + \langle g, \vy -\vz \rangle, &   
        \end{cases}
    \end{equation}
    \begin{align*}
        (1-\alpha)f(\vx) + \alpha f(\vy) \ge f(\vz) + \langle  \rangle
    \end{align*}
\end{proof}

\begin{remark}
    The opposite doesn't hold in general.
    \begin{equation}
        f(\vx) :=
        \begin{cases} 
        -(x)^{-\frac{1}{2}}, & x \ge 0  \\ 
        +\infty, &   o.w.
        \end{cases}
    \end{equation}
    \begin{claim}
        $f$ is convex but not subdiffrentiable at $x = 0$
    \end{claim}
    \begin{proof}
        Suppose f.p.o.c $\exists g \in \partial f(0) \iff \forall \, y \in \mathbb{R}:
        \, f(y) \ge gy$.

        In particular, $\forall \, y \ge 0: -\sqrt{y} \ge gy$.

        Consider 
        \begin{enumerate}
            \item $y = 1$
            \item $y = \frac{1}{2g^{2}}$
        \end{enumerate}
    \end{proof}
\end{remark}

\begin{remark}
    For the interior of the domain, the opposite holds.
\end{remark}


\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be a proper convex function and let $\vx \in intdom(f)$.
    Then $\partial f(\vx) \neq \emptyset$ and $\partial f(\vx)$ is bounded.
\end{theorem}


\subsubsection{Relationship between dir der and subgradient}
\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty, +\infty]$ be a proper convex function. Then for any 
    $\vx \in intdom(f)$ and $\vz \in \mathbb{E}$
    \begin{align*}
        f'(\vx; \vz)  = \max_{g}\{ \langle g,\vz \rangle: \, g \in \partial f(\vx) \}
    \end{align*}
\end{theorem}
\begin{proof}
    Fix $\vx \in intdom(f)$ and $\vz \in \mathbb{E}$.
    
    \begin{align*}
        \forall g \in \partial f(\vx),\, \forall \vy \in \mathbb{E}: \; 
        f(\vy) \ge f(\vx) + \langle g, \vy-\vx \rangle\\
        f'(\vx; \vz) = \lim_{\alpha \to 0^{+}} \frac{f(\vx + \alpha \vz) - f(\vx)}{\alpha} 
        \ge \lim_{\alpha \to 0^{+}}
    \end{align*}
    Let $h(\vw) := f'(\vx; \vw)$. We know $h$ is convex, real-valued, positively homogeneous. $\Longrightarrow h$ is 
    subdifferentiable on $\mathbb{E}$.

    Let $\bar{g} \in \partial h(\vz)$. Then $\forall \vv \in \mathbb{E} \; \forall \alpha \ge 0$.
    \begin{align*}
        \alpha f'(\vx;\vv) = f'(\vx; \alpha \vv) = h(\alpha \vv) \\
        \ge h(\vz) + \langle \bar{g}, \alpha \vv - \vz \rangle \\
        = f'(\vx;\vz) + \langle \bar{g}, \alpha \vv - \vz \rangle
    \end{align*}
\end{proof}

\DATE{Oct 2, 2024}

Quick recap:
\begin{enumerate}
    \item $f'(\vx; \vz) = \lim_{x \to 0+}\frac{f(\vx + \alpha\vz) - f(\vx)}{\alpha}$
    \item $\partial f(\vx) = \{ g \in \mathbb{E}^{*}: f(\vy) \ge f(\vx) + \langle g,\vy-\vx \rangle
    ,\; \forall \vy \in \mathbb{E} \}$
\end{enumerate}

\subsection{Differentiability}
\begin{definition}
    Let $f: \mathbb{E} \to (-\infty, \infty]$ and $\vx \in intdom(f)$. The function $f$ is said to be 
    (Frechel) differentiable at $\vx$ if $\exists g \in \mathbb{E}^{*}$ s.t.
    \begin{align} \label{def:differentiability}
        \lim_{\vh \to \vec{0}} \frac{f(\vx + \vh) - f(\vx) - \langle g,\vh \rangle}{\| \vh \| } = 0
    \end{align}
    The unique vector $g$ satisfying this limit is called the gradient of $f$ at $\vx$, and is denoted 
    by $\nabla f(\vx)$.
\end{definition}

\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be proper and suppose that 
    $f$ is differentiable at $\vx \in intdom(f)$. Then :
    \begin{align*}
        \forall \, \vz \in \mathbb{E}: \; 
        f'(\vx; \vz) = \langle \nabla f(\vx), \vz \rangle
    \end{align*}
\end{theorem}
\begin{proof}
    It's trivial for $\vz = \vec{0}$, so assume that $\vz \neq \vec{0}$.

    Now start with Eq.~\eqref{def:differentiability} with $h = \alpha \vz$ where $\vz$ is some unit 
    vector, 
    \begin{align*}
        0 = \lim_{\alpha \to 0+}\frac{f(\vx + \alpha\vz) - f(\vx) - \langle \nabla f(\vx), \alpha \vz \rangle}
        {\alpha} = \lim_{\alpha \to 0+}\frac{f(\vx + \alpha \vz) - f(\vx)}{\alpha} 
            - \langle \nabla f(\vx), \vz \rangle
    \end{align*}
\end{proof}

\begin{remark}
    What is the gradient?
    \begin{enumerate}
        \item $\mathbb{E} = \mathbb{R}^{d}$, $\langle \vx, \vy \rangle = \vx^{\top}\vy$
        Take $\vz = e_{i}$, we have 
        \begin{align*}
            \frac{\partial f}{\partial \vx_{i}}(\vx) = f'(\vx;e_{i}) = \nabla f(\vx)^{\top}e_i
                =(\nabla f(\vx))_{i}
        \end{align*}
        since we know:
        \begin{align*}
            \nabla f(\vx) = D_{f}(\vx) = \begin{bmatrix} \frac{\partial f}{\partial \vx_1}(\vx) \\ 
                \frac{\partial f}{\partial \vx_2}(\vx) \\ \vdots 
                \\ 
                \frac{\partial f}{\partial \vx_d}(\vx)
             \end{bmatrix}
        \end{align*}
        \begin{align*}
            f'(\vx; \vz) = D_{f}(\vx)^{\top}\vz = \sum_{i=1}^{d}\frac{\partial f(\vx)}{\partial \vx_{i}}\vz_{i}
        \end{align*}
    \end{enumerate}
    \item $\mathbb{E} = \mathbb{R}^{d}$, $\langle \vx,\vy \rangle = \vx^{\top}M\vy$, 
    $M \in \mathbb{S}^{d}$, $M$ is positive-definite.
    \begin{align*}
        (\nabla f(\vx))_{i} = \nabla f(\vx)^{\top}e_{i} = \nabla f(\vx)^{\top}MM^{-1}e_{i}\\
        = \langle \nabla f(\vx), M^{-1}e_{i} \rangle\\
        = f'(M; M^{-1}e_{i}) \\
        = D_{f}(\vx)^{\top}M^{-1}e_{i}
    \end{align*}
    where $M = \sum_{i=1}^{d}\lambda_i u_i u_i^{\top}$
    \item $\mathbb{E} = \mathbb{R}^{n \times d}$, $\langle X, Y \rangle = Tr(X^{\top}Y)$
    \begin{align*}
        \nabla f(X) = D_{f}(X) = 
    \end{align*}
    \item $\mathbb{E} = \mathbb{R}^{n \times d}$, $\langle X,Y \rangle = 
    Tr(X^{\top}MY)$, $M \in \mathbb{S}^{d}$, M is PD. Then $\nabla f(\vx) = M^{-1}D_{f}(X)$
\end{remark}

\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty, \infty]$ be a proper convex function and let 
    $\vx \in intdom(f)$. If $f$ is differentiable at $\vx$, then $\partial f(\vx) = \{ \nabla f(\vx) \}$.
    Conversely, if $f$ has unique subgradient at $\vx$, then it is differentiable at $\vx$ and 
    $\partial f(\vx) = \{ \nabla f(\vx) \}$.
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item ($\Longrightarrow$) 
        $f$ is differentiable at $\vx \in intdom(f)$. $f$ is proper, convex and $\vx \in intdom(f)
        \Longrightarrow \partial f(\vx) \neq \emptyset$. let $g \in \partial f(\vx)$. To show:
        $g = \nabla f(\vx)$. By the (MF), we have:
        \begin{align*}
            \langle \nabla f(\vx), \vz \rangle = 
                f'(\vx; \vz) = \max_{\tilde{g} \in \partial f(\vx)}\langle \tilde{g}, \vz \rangle
                \ge \langle g, \vz \rangle \quad \forall  \vz \in \mathbb{E}\\
            \Longrightarrow \langle g - \nabla f(\vx), \vz \rangle \le 0 \quad \forall \, \vz \in \mathbb{E}
        \end{align*}
        We can take $\vz = g - \nabla f(\vx)$ or we can think of the dual norm:
        \begin{align*}
            \sup_{\vz \in \mathbb{E}, \| \vz \|\le 1 }\langle g - \nabla f(\vx), \vz \rangle \le 0\\
            \Longrightarrow \| g - \nabla f(\vx) \| _{*} = 0
        \end{align*}
    \end{enumerate}
\end{proof}

Why does subgradient matter?
\begin{definition}[Locally Lipshitz continuous]
    $\forall $ compact $C \subseteq \mathbb{E}$, $\exists L \in (0,\infty)$ s.t.
    \begin{align*}
        \forall \vx, \vy \in C: |f(\vx) - f(\vy)| \le L\| \vx - \vy \| 
    \end{align*}
\end{definition}
\begin{theorem}[Rademacher's theorem]
    If $f$ is locally Lipshitz continuous, then $f$ is differentiable a.e..
\end{theorem}
Even though this theorem holds, we still need to think about non-differentiability by some reasons 
about like critical points...
\begin{definition}[Clarke's subdiff]
    
\end{definition}
For every non-differentiable point, we have a range of domain where we have gradients. We take 
convex combination of them.

\subsection{Subgradient of Lipschitz function}
\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ be proper and convex. Suppose that $C \subseteq intdom(f)$.
    Consider the following two statements:
    \begin{enumerate}
        \item $|f(\vx) - f(\vy)| \le L\| \vx - \vy \|, \quad \forall \, \vx, \vy \in C$
        \item $\| g \|_{*} \le L\quad \forall \vx \in C, g \in \partial f(\vx)$
    \end{enumerate}
    Then
    \begin{enumerate}
        \item (ii.) $\Longrightarrow$ (i.).
        \item If $C$ is open, then $(i.) \iff (ii.)$.
    \end{enumerate}
\end{theorem}




\DATE{Oct 4, 2024}

\section{Conjugate Function}
\begin{definition}
    Let $f: \mathbb{E} \to [-\infty,\infty]$. The function $f^{*}: \mathbb{E}^{*} \to [-\infty,\infty]$,
    defined by 
    \begin{align*}
        f^{*}(\vy) = \sup_{\vx \in \mathbb{E}}\{ \langle \vy,\vx \rangle - f(\vx) \} \quad \forall \vy \in \mathbb{E}^{*}
    \end{align*}
    is called the conjugate function of $f$.
\end{definition}
\begin{remark}
    There are some other names for it: convex conjugate, Fenchel conjugate, legendre transform, Fenchel
    -Legendre transform.
\end{remark}

\begin{example}
    Let $f = \delta_{C}$.
    \begin{equation} \delta_{C}(\vx) = 
        \begin{cases} 
        0, & \vx  \in C  \\ 
        +\infty, & \vx  \notin C 
        \end{cases}
    \end{equation}
    where $C \subseteq  \mathbb{E}, C \neq \emptyset$.

    $\forall \vy \in \mathbb{E}^{*}$, we have
    \begin{align*}
        f^{*}(\vy) =\;& \sup_{\vx \in \mathbb{E}}\{ \langle \vy,\vx \rangle - \delta_{C}(\vx) \}\\
        = \;& \sup_{\vx \in C}\langle \vy,\vx \rangle = \sigma_{C}(\vy)
    \end{align*}
\end{example}

\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty, \infty]$. Then $f^{*}$ is closed and convex.
\end{theorem}

\begin{example}
    $f(\vx) = \frac{1}{2}\| \vx \| ^{2} + \delta_{C}(\vx)$ where $\| \cdot \| = \sqrt{\langle \cdot ,\cdot  \rangle} $

    Then we have:
    \begin{align*}
        f^{*}(\vy) = \sup_{\vx \in \mathbb{E}}\{ \langle \vy, \vx \rangle - \frac{1}{2}\| \vx \|^{2}  
            - \delta_{C}(\vx)\}\\
        = \sup_{\vx \in C}\{ \langle \vy, \vx \rangle - \frac{1}{2}\| \vx \|^{2} + \frac{1}{2}\| \vy \| ^{2} \}\\
        = \frac{1}{2}\| \vy \| ^{2} + \sup_{\vx \in C}\{ -\frac{1}{2}\| \vx - \vy \|^{2}  \}\\
        = \frac{1}{2}\| \vy \| ^{2} - \frac{1}{2}d_{C}^{2}(\vy)
    \end{align*}
\end{example}

\begin{theorem}[Conjugate of proper convex functions are proper convex]
    Let $f: \mathbb{E} \to (-\infty,\infty] $ be proper and convex. Then 
    $f^{*}: \mathbb{E}^{*} \to (-\infty,\infty]$ is proper.
\end{theorem}
\begin{proof}
    $\forall \vy \in \mathbb{E}^{*}: \; f^{*}(\vy) > -\infty$.
    We know $f \text{ is } proper \Longrightarrow \exists \hat{\vx} \text{ s.t. } f(\hat{\vx}) < \infty$.
    \begin{align*}
        f^{*}(\vy   ) = \sup_{\vx \in \mathbb{E}}\{ \langle \vx,\vy \rangle - f(\vx) \} 
            \ge \langle \hat{\vx},y \rangle - f(\hat{\vx}) > -\infty
    \end{align*}
    \begin{lemma}
        For any proper convex function, $\exists \vx \in dom(f)$ s.t. 
        $\partial f(\vx) \neq \emptyset$.
    \end{lemma}
    $\Longrightarrow$ we can choose some $\vx \in dom(f)$ and $g \in \partial f(\vx)$.
    \begin{align*}
        \forall  \vz \in \mathbb{E}: f(\vz) \ge f(\vx) + \langle g, \vz - \vx \rangle\\
        \Longrightarrow \langle g,\vx \rangle - f(\vx) \ge \langle g,\vz \rangle - f(\vz)
    \end{align*}
    We have 
    \begin{align*}
        f^{*}(g) = \sup_{\vz \in \mathbb{E}}\{ \langle g,\vz \rangle - f(\vz)\} 
            \le \langle g,\vx \rangle -f(\vx) < \infty
    \end{align*}
\end{proof}

\begin{theorem}[Fenchel Inequality]
    Given $f: \mathbb{E} \to (-\infty,\infty], f$ is proper.

    $\forall \vx \in \mathbb{E}, \vy \in \mathbb{E}^{*}$:
    \begin{align*}
        f(\vx) + f^{*}(\vy) \ge \langle \vx ,\vy\rangle 
        \iff f^{*}(\vy) \ge \langle \vx,\vy \rangle - f(\vx)
    \end{align*}
\end{theorem}

\subsection{The Biconjugate}
$(\mathbb{E}^{**} = \mathbb{E})$

\begin{definition}
    \begin{align*}
        f^{**}(\vx) = \sup_{\vy \in \mathbb{E}^{*}}\{ \langle \vy,\vx \rangle - f^{*}(\vy) \}
    \end{align*}
\end{definition}

\begin{lemma}[$f^{** }\le f$]
    Let $f: \mathbb{E} \to [-\infty,\infty]$. Then $f(\vx) \ge f^{**}(\vx), \; \forall \vx \in \mathbb{E}$.
\end{lemma}
\begin{proof}
    $\forall \vx \in \mathbb{E}, \forall \vy \in \mathbb{E}^{*}$
    \begin{align*}
        f^{*}(\vy) \ge \langle \vy,\vx \rangle - f(\vx) \\
        \iff f(\vx) \ge \langle \vy, \vx \rangle - f^{*}(\vy) \\
        f(\vx) \ge \sup_{\vy \in \mathbb{E}^{*}}\{ \langle \vy,\vx \rangle - f^{*}(\vy) \}
            = f^{**}(\vx)
    \end{align*}
\end{proof}

\begin{theorem}[$f^{**} = f$ whenever $f$ is proper, closed and convex]
    
\end{theorem}

\begin{example}
    \begin{enumerate}
        \item Conjugate of support functions: 
        $C \subseteq \mathbb{E}, \, C \neq \emptyset$.
        $\underbrace{cl}_{\text{closure}}(\underbrace{conv(C)}_{\text{convex hull of }C})$ is closed 
        and convex.
        \begin{align*}
            \underbrace{\delta_{cl(conv(C))}}_{\text{closed and convex fn}} \equiv 
                \delta_{cl(conv(C))} \equiv (\delta_{cl(conv(C))}^{*})^{*}\\
            = \sigma_{cl(conv(C))}^{*} = \sigma_{C}^{*}
        \end{align*}
        \item ($\mathbb{E} = \mathbb{R}^{d}$)
        \begin{align*}
            f(\vx) &= \max_{1 \le i \le d} x_{i} \\
            &= \max_{\vv \in \Delta_{d}}\vv^{\top}\vx = \sigma_{\Delta_{d}}(\vx)
        \end{align*}
        where $\Delta_{d} = \{ \vv \ge 0:\1^{\top}\vv = 1 \}$.

        We have 
        \begin{align*}
            f^{*}(\vy) = \sigma_{\Delta_{d}}^{*}(\vy) = \delta_{\Delta_{d}}(\vy)
        \end{align*}
        \item $f(\vx) = \frac{1}{2}\| \vx \|^{2} - \frac{1}{2}d_{C}^{2}(\vx) $, where 
        $\| \cdot  \| = \sqrt{\langle \cdot ,\cdot  \rangle} , \, C \neq \emptyset, C \subseteq \mathbb{E}$,
        $C$ is closed and convex, we have 
        \begin{align*}
            f(\vx) = g^{*}(\vx), \; g(\vy) = \frac{1}{2}\| \vy \| ^{2} + \delta_{C}(\vy)
        \end{align*}
        where $g(\vy)$ is closed and convex, so 
        \begin{align*}
            f^{*}(\vy) = g^{**}(\vy) = g(\vy)
        \end{align*}
    \end{enumerate}
\end{example}

\subsection{Conjugate Calculus Rules:}

\begin{theorem}[Conjugate of a separable function]
    Let $g: \mathbb{E}_1 \times \mathbb{E}_2 \times \cdots \times \mathbb{E}_m \to (-\infty,\infty]$ be 
    given by $g(\vx_1,\vx_2, \ldots ,\vx_m) = \sum_{i=1}^{m}f_i(\vx_{i}),\, \vx_{i} \in \mathbb{E}_i$
    where $f_i: \mathbb{E}_i \to (-\infty,\infty],\, i \in \{ 1, \ldots ,m \}$.
    Then:
    \begin{align*}
        g^{*}(\vy_1, \ldots ,\vy_{m}) = \sum_{i=1}^{m}f_{i}^{*}(\vy_{i}), \; \forall \,\vy_{i} \in \mathbb{E}_{i},
            i \in \{ 1, \ldots ,m \}
    \end{align*}
\end{theorem}

\begin{theorem}[Conjugate of $f(A(\vx - \va)) + \langle \vb,\vx \rangle + c$]
    Let $f: \mathbb{E} \to (-\infty,\infty]$ and let $A: \mathbb{V} \to \mathbb{E}$ be an invertible 
    linear transformation. $\va \in \mathbb{V},\, \vb \in \mathbb{V}^{*},\, c\in \mathbb{R}$. 
    Then the conjugate of $f(A(\vx - \va)) + \langle \vb, \vx \rangle + c := g(\vx)$ is 
    \begin{align*}
        g^{*}(\vx) = f^{*}((A^{\top})^{-1}(\vy - \vb)) + \langle \va, \vy \rangle - c - \langle \va, \vb \rangle,
        \; \forall \vy \in \mathbb{V}^{*}
    \end{align*}
\end{theorem}
\begin{proof}
    Let $\vz = A(\vx - \va), \, \vx = \va + A^{-1}(\vz)$. We have 
    \begin{align*}
        g^{*}(\vy) = \sup_{\vx \in \mathbb{V}}\{ \langle \vy,\vx \rangle - f(A(\vx - \va)) - 
        \langle \vb, \vx \rangle - c \} \\
        = -c + \langle \va, \vy - \vb \rangle + 
            \sup_{\vz \in \mathbb{V}}\{ \langle A^{-1}(\vz), \vy - \vb \rangle - f(\vz) \}\\
        = \sup_{\vz \in \mathbb{V}}\{ \langle \vz, (A^{-1})^{\top}(\vy - \vb) \rangle - f(\vz) \}\\
        = f^{*}((A^{\top})^{-1}(\vy - \vb))
    \end{align*}
\end{proof}

\begin{theorem}
    Let $f: \mathbb{E} \to (-\infty,\infty]$ and let $\alpha > 0$
    \begin{enumerate}
        \item let $g(\vx) = \alpha f(\vx)$, we have 
        \begin{align*}
            g^{*}(\vy) = \alpha f^{*}(\frac{\vy}{\alpha}), \; \vy \in \mathbb{E}^{*}
        \end{align*}
        \item let $h(\vx) = \alpha f(\frac{\vx}{\alpha})$, we have 
        \begin{align*}
            h^{*}(\vy) = \alpha f^{*}(\vy), \; \vy \in \mathbb{E}^{*}
        \end{align*}
    \end{enumerate}
\end{theorem}




\end{document}